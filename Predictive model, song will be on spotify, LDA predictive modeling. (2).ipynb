{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17bf2b88",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb616f",
   "metadata": {},
   "source": [
    "# import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6635e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Eduardo/OneDrive/Documents/Masters python/Machine learning/spotify_listenership_sample (1).csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7e7868",
   "metadata": {},
   "source": [
    "# Data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25723549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id            track_name          track_artist  \\\n",
      "0  1iTZnsVqhCBdZqp9gjITDB            Run To You               Code 64   \n",
      "1  42LiXQn3xibOEodtBMoJp3  Drop The Bomb On 'Em                Eminem   \n",
      "2  2cTGafydY9vdnqJQ2Gw6t8           Butterflies                  Myon   \n",
      "3  2boJnT3S2aSBagFEUTXrfx      Get Your Roll On            Big Tymers   \n",
      "4  5HzekjQF7xLqBVeynqprDk          By Your Side  Ballin Entertainment   \n",
      "\n",
      "  track_album_release_date  track_duration_ms  sound_danceability  \\\n",
      "0                6/16/2006             267747               0.445   \n",
      "1                5/15/2009             287267               0.892   \n",
      "2                6/22/2018             234375               0.539   \n",
      "3                 1/1/2007             245200               0.849   \n",
      "4                4/20/2008             198165               0.802   \n",
      "\n",
      "   sound_energy  sound_key  sound_loudness  sound_mode  ...  sound_liveness  \\\n",
      "0         0.999          0          -5.627           1  ...          0.0534   \n",
      "1         0.693         11          -4.048           0  ...          0.1230   \n",
      "2         0.912          7          -3.113           1  ...          0.2520   \n",
      "3         0.812          7          -5.005           1  ...          0.1090   \n",
      "4         0.823          3          -9.799           0  ...          0.2640   \n",
      "\n",
      "   sound_valence  sound_tempo  genre_edm  genre_latin  genre_pop  genre_rnb  \\\n",
      "0          0.195      139.996          0            0          1          0   \n",
      "1          0.357       91.989          0            0          0          0   \n",
      "2          0.233      128.016          1            0          0          0   \n",
      "3          0.454       88.930          0            0          0          0   \n",
      "4          0.964      130.771          0            1          0          0   \n",
      "\n",
      "   genre_rap  genre_rock  has_listenership  \n",
      "0          0           0                 0  \n",
      "1          1           0                 0  \n",
      "2          0           0                 0  \n",
      "3          1           0                 0  \n",
      "4          0           0                 0  \n",
      "\n",
      "[5 rows x 23 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# import pandas to look at data\n",
    "\n",
    "\n",
    "#read file \n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# let's look at the first few rows of the dataset\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e609bb",
   "metadata": {},
   "source": [
    "non-predictive variables:\n",
    "- track_id\n",
    "- track_name\n",
    "- track_artist\n",
    "- track_album_release_date\n",
    "\n",
    "Binary variables\n",
    "- genre_edm\n",
    "- genre_latin\n",
    "- genre_pop\n",
    "- genre_rnb\n",
    "- genre_rap\n",
    "- genre_rock\n",
    "- has_leistenership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4fbce5fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2999 entries, 0 to 2998\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   track_id                  2999 non-null   object \n",
      " 1   track_name                2999 non-null   object \n",
      " 2   track_artist              2999 non-null   object \n",
      " 3   track_album_release_date  2999 non-null   object \n",
      " 4   track_duration_ms         2999 non-null   int64  \n",
      " 5   sound_danceability        2999 non-null   float64\n",
      " 6   sound_energy              2999 non-null   float64\n",
      " 7   sound_key                 2999 non-null   int64  \n",
      " 8   sound_loudness            2999 non-null   float64\n",
      " 9   sound_mode                2999 non-null   int64  \n",
      " 10  sound_speechiness         2999 non-null   float64\n",
      " 11  sound_acousticness        2999 non-null   float64\n",
      " 12  sound_instrumentalness    2999 non-null   float64\n",
      " 13  sound_liveness            2999 non-null   float64\n",
      " 14  sound_valence             2999 non-null   float64\n",
      " 15  sound_tempo               2999 non-null   float64\n",
      " 16  genre_edm                 2999 non-null   int64  \n",
      " 17  genre_latin               2999 non-null   int64  \n",
      " 18  genre_pop                 2999 non-null   int64  \n",
      " 19  genre_rnb                 2999 non-null   int64  \n",
      " 20  genre_rap                 2999 non-null   int64  \n",
      " 21  genre_rock                2999 non-null   int64  \n",
      " 22  has_listenership          2999 non-null   int64  \n",
      "dtypes: float64(9), int64(10), object(4)\n",
      "memory usage: 539.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Let's look If the variables have any Null Values and what data type is it\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bdc05b",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b5a5a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# We use Standard Scaler before feeding it into the model\n",
    "\n",
    "#Normalize numerical features\n",
    "# select numerical features you want to normalize (exlude the binary variables and the non-predictive features).  \n",
    "\n",
    "numerical_features = data[['track_duration_ms', 'sound_danceability', 'sound_energy', 'sound_key',\n",
    "                          'sound_loudness', 'sound_speechiness', 'sound_acousticness',\n",
    "                          'sound_instrumentalness', 'sound_liveness', \n",
    "                          'sound_instrumentalness', 'sound_liveness','sound_valence', 'sound_tempo']]\n",
    "# Create the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#scale the numerical features\n",
    "data[numerical_features.columns] = scaler.fit_transform(numerical_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035f4c5f",
   "metadata": {},
   "source": [
    "prepare for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c6986822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions: (2399, 18) (2399,)\n",
      "Testing set dimensions: (600, 18) (600,)\n"
     ]
    }
   ],
   "source": [
    "#split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Separate features from the target variable\n",
    "\n",
    "#Drop the non-predictive columns \n",
    "x = data.drop(['track_id', 'track_name', 'track_artist', 'track_album_release_date', 'has_listenership'], axis = 1)\n",
    "\n",
    "# Define Target variable\n",
    "y= data['has_listenership']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# confirm the split\n",
    "print(\"Training set dimensions:\", x_train.shape, y_train.shape)\n",
    "print(\"Testing set dimensions:\", x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dba054e",
   "metadata": {},
   "source": [
    "# Data modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1a2657",
   "metadata": {},
   "source": [
    "SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e98c7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import then necessary library for SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Create an SVM classifier\n",
    "svm_model = SVC(kernel = 'rbf', C=1.0, gamma ='auto', random_state = 42) \n",
    "\n",
    "# Fit the model in the training data \n",
    "svm_model.fit(x_train, y_train)\n",
    "\n",
    "#Predict on the training and testing data\n",
    "svm_train_predictions = svm_model.predict(x_train)\n",
    "svm_test_predictions = svm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "698c235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.62      0.65      1184\n",
      "           1       0.66      0.73      0.70      1215\n",
      "\n",
      "    accuracy                           0.68      2399\n",
      "   macro avg       0.68      0.68      0.68      2399\n",
      "weighted avg       0.68      0.68      0.68      2399\n",
      "\n",
      "Training Accuracy: 0.6765318882867861\n",
      "\n",
      "SVM Testing Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.53      0.58       315\n",
      "           1       0.56      0.67      0.61       285\n",
      "\n",
      "    accuracy                           0.60       600\n",
      "   macro avg       0.60      0.60      0.60       600\n",
      "weighted avg       0.60      0.60      0.60       600\n",
      "\n",
      "Testing Accuracy: 0.5966666666666667\n",
      "\n",
      "Confusion Matrix:\n",
      "[[168 147]\n",
      " [ 95 190]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluation metrics\n",
    "\n",
    "# Import the necessary libraries \n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "print(\"SVM Training Performance:\")\n",
    "print(classification_report(y_train, svm_train_predictions))\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, svm_train_predictions))\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "print(\"\\nSVM Testing Performance:\")\n",
    "print(classification_report(y_test, svm_test_predictions))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, svm_test_predictions))\n",
    "\n",
    "# Output the confusion matrix for the testing set\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, svm_test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e58ab",
   "metadata": {},
   "source": [
    "The test are 67% and 59% accurate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa57c1aa",
   "metadata": {},
   "source": [
    "# textual features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9974fb5f",
   "metadata": {},
   "source": [
    "Bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0188eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary library\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "#create the vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform 'track_name' to create the Bag-of-Words feature matrix\n",
    "X_bow = vectorizer.fit_transform(data['track_name'])\n",
    "\n",
    "# Convert to DataFrame (optional, for better integration with pandas)\n",
    "X_bow_df = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22002d51",
   "metadata": {},
   "source": [
    "Tf - IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6f66af4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create the tfidVfectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform 'track_name' to create the TF-IDF feature matrix\n",
    "x_tfidf = tfidf.fit_transform(data['track_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0cbe8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the tfidf_matrix, which is a sparse matrix, to a DataFrame\n",
    "tfidf_features = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Combine the numerical features with the TF-IDF features\n",
    "# Ensure that indices are aligned between the dataframes\n",
    "combined_features = pd.concat([numerical_features.reset_index(drop=True), tfidf_features.reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c79829",
   "metadata": {},
   "source": [
    "# Combine datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "61ee521a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine numerical features with BoW features\n",
    "X_combined_bow = pd.concat([numerical_features.reset_index(drop=True), X_bow_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Combine numerical features with TF-IDF features\n",
    "X_combined_tfidf = pd.concat([numerical_features.reset_index(drop=True), tfidf_features.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Define the target variable (assuming it's a binary classification problem)\n",
    "y = data['has_listenership']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3696457c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for BoW features\n",
    "X_train_bow, X_test_bow, y_train, y_test = train_test_split(X_combined_bow, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Split data for TF-IDF features\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_combined_tfidf, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea00830",
   "metadata": {},
   "source": [
    "# Modeling with textual features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5da8f3",
   "metadata": {},
   "source": [
    "SVM model again (with bag of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "66b0331d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Performance (BoW):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93      1184\n",
      "           1       0.93      0.94      0.94      1215\n",
      "\n",
      "    accuracy                           0.94      2399\n",
      "   macro avg       0.94      0.94      0.94      2399\n",
      "weighted avg       0.94      0.94      0.94      2399\n",
      "\n",
      "Training Accuracy: 0.9353897457273864\n",
      "\n",
      "SVM Testing Performance (BoW):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60       315\n",
      "           1       0.54      0.47      0.51       285\n",
      "\n",
      "    accuracy                           0.56       600\n",
      "   macro avg       0.56      0.56      0.55       600\n",
      "weighted avg       0.56      0.56      0.56       600\n",
      "\n",
      "Testing Accuracy: 0.56\n",
      "\n",
      "Confusion Matrix (BoW):\n",
      "[[201 114]\n",
      " [150 135]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "# Initialize the SVM model with RBF kernel\n",
    "svm_bow = SVC(kernel='rbf', C=1.0, gamma='auto', random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict on the training and testing data\n",
    "svm_train_predictions_bow = svm_bow.predict(X_train_bow)\n",
    "svm_test_predictions_bow = svm_bow.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "print(\"SVM Training Performance (BoW):\")\n",
    "print(classification_report(y_train, svm_train_predictions_bow))\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, svm_train_predictions_bow))\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "print(\"\\nSVM Testing Performance (BoW):\")\n",
    "print(classification_report(y_test, svm_test_predictions_bow))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, svm_test_predictions_bow))\n",
    "\n",
    "# Output the confusion matrix for the testing set\n",
    "print(\"\\nConfusion Matrix (BoW):\")\n",
    "print(confusion_matrix(y_test, svm_test_predictions_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861cd1ee",
   "metadata": {},
   "source": [
    "SVM model (with tf-idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "44df1a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Performance (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93      1184\n",
      "           1       0.93      0.94      0.94      1215\n",
      "\n",
      "    accuracy                           0.93      2399\n",
      "   macro avg       0.94      0.93      0.93      2399\n",
      "weighted avg       0.93      0.93      0.93      2399\n",
      "\n",
      "Training Accuracy: 0.9349729053772405\n",
      "\n",
      "SVM Testing Performance (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.64      0.60       315\n",
      "           1       0.54      0.47      0.51       285\n",
      "\n",
      "    accuracy                           0.56       600\n",
      "   macro avg       0.56      0.56      0.55       600\n",
      "weighted avg       0.56      0.56      0.56       600\n",
      "\n",
      "Testing Accuracy: 0.56\n",
      "\n",
      "Confusion Matrix (TF-IDF):\n",
      "[[201 114]\n",
      " [150 135]]\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVM model with RBF kernel\n",
    "svm_tfidf = SVC(kernel='rbf', C=1.0, gamma='auto', random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the training and testing data\n",
    "svm_train_predictions_tfidf = svm_tfidf.predict(X_train_tfidf)\n",
    "svm_test_predictions_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "print(\"SVM Training Performance (TF-IDF):\")\n",
    "print(classification_report(y_train, svm_train_predictions_tfidf))\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, svm_train_predictions_tfidf))\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "print(\"\\nSVM Testing Performance (TF-IDF):\")\n",
    "print(classification_report(y_test, svm_test_predictions_tfidf))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, svm_test_predictions_tfidf))\n",
    "\n",
    "# Output the confusion matrix for the testing set\n",
    "print(\"\\nConfusion Matrix (TF-IDF):\")\n",
    "print(confusion_matrix(y_test, svm_test_predictions_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c599d0",
   "metadata": {},
   "source": [
    "Logistic regression (with bag-of-words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "daf7b974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Performance (BoW):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66      1184\n",
      "           1       0.00      0.00      0.00      1215\n",
      "\n",
      "    accuracy                           0.49      2399\n",
      "   macro avg       0.25      0.50      0.33      2399\n",
      "weighted avg       0.24      0.49      0.33      2399\n",
      "\n",
      "Training Accuracy: 0.49353897457273865\n",
      "\n",
      "Logistic Regression Testing Performance (BoW):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       315\n",
      "           1       0.00      0.00      0.00       285\n",
      "\n",
      "    accuracy                           0.53       600\n",
      "   macro avg       0.26      0.50      0.34       600\n",
      "weighted avg       0.28      0.53      0.36       600\n",
      "\n",
      "Testing Accuracy: 0.525\n",
      "\n",
      "Confusion Matrix (BoW):\n",
      "[[315   0]\n",
      " [285   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg_bow = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg_bow.fit(X_train_bow, y_train)\n",
    "\n",
    "# Predict on the training and testing data\n",
    "log_reg_train_predictions_bow = log_reg_bow.predict(X_train_bow)\n",
    "log_reg_test_predictions_bow = log_reg_bow.predict(X_test_bow)\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "print(\"Logistic Regression Training Performance (BoW):\")\n",
    "print(classification_report(y_train, log_reg_train_predictions_bow, zero_division=0))\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, log_reg_train_predictions_bow))\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "print(\"\\nLogistic Regression Testing Performance (BoW):\")\n",
    "print(classification_report(y_test, log_reg_test_predictions_bow, zero_division=0))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, log_reg_test_predictions_bow))\n",
    "\n",
    "# Output the confusion matrix for the testing set\n",
    "print(\"\\nConfusion Matrix (BoW):\")\n",
    "print(confusion_matrix(y_test, log_reg_test_predictions_bow))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af829c",
   "metadata": {},
   "source": [
    "Logistic regression (with TF-Idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c25010d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Training Performance (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      1.00      0.66      1184\n",
      "           1       0.00      0.00      0.00      1215\n",
      "\n",
      "    accuracy                           0.49      2399\n",
      "   macro avg       0.25      0.50      0.33      2399\n",
      "weighted avg       0.24      0.49      0.33      2399\n",
      "\n",
      "Training Accuracy: 0.49353897457273865\n",
      "\n",
      "Logistic Regression Testing Performance (TF-IDF):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      1.00      0.69       315\n",
      "           1       0.00      0.00      0.00       285\n",
      "\n",
      "    accuracy                           0.53       600\n",
      "   macro avg       0.26      0.50      0.34       600\n",
      "weighted avg       0.28      0.53      0.36       600\n",
      "\n",
      "Testing Accuracy: 0.525\n",
      "\n",
      "Confusion Matrix (TF-IDF):\n",
      "[[315   0]\n",
      " [285   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression model\n",
    "log_reg_tfidf = LogisticRegression(max_iter=1000, random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Predict on the training and testing data\n",
    "log_reg_train_predictions_tfidf = log_reg_tfidf.predict(X_train_tfidf)\n",
    "log_reg_test_predictions_tfidf = log_reg_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model's performance on the training set\n",
    "print(\"Logistic Regression Training Performance (TF-IDF):\")\n",
    "print(classification_report(y_train, log_reg_train_predictions_tfidf, zero_division=0))\n",
    "print(\"Training Accuracy:\", accuracy_score(y_train, log_reg_train_predictions_tfidf))\n",
    "\n",
    "# Evaluate the model's performance on the testing set\n",
    "print(\"\\nLogistic Regression Testing Performance (TF-IDF):\")\n",
    "print(classification_report(y_test, log_reg_test_predictions_tfidf, zero_division=0))\n",
    "print(\"Testing Accuracy:\", accuracy_score(y_test, log_reg_test_predictions_tfidf))\n",
    "\n",
    "# Output the confusion matrix for the testing set\n",
    "print(\"\\nConfusion Matrix (TF-IDF):\")\n",
    "print(confusion_matrix(y_test, log_reg_test_predictions_tfidf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143c6d70",
   "metadata": {},
   "source": [
    "# Topic Modeling (LDA) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd53757d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0:\n",
      "stay paradise album time yo el live remix remastered version\n",
      "Topic #1:\n",
      "dance extended sweet te life light night original mix remix\n",
      "Topic #2:\n",
      "big make want da got girl way radio edit feat\n",
      "Topic #3:\n",
      "forever tú ya black lost man day world feat good\n",
      "Topic #4:\n",
      "gonna feel wanna remaster don let remix la like love\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    " #Step 1: Use CountVectorizer to transform the song titles into a matrix of token counts\n",
    "cv = CountVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "dtm = cv.fit_transform(data['track_name'])  # Document-term matrix\n",
    "\n",
    "# Step 2: Fit LDA to the document-term matrix\n",
    "n_components = 5  \n",
    "lda = LatentDirichletAllocation(n_components=n_components, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Step 3: Print the top words in each topic\n",
    "words = cv.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic #{topic_idx}:\")\n",
    "    print(\" \".join([words[i] for i in topic.argsort()[-10:]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33baae1b",
   "metadata": {},
   "source": [
    "Identify Thematic modeling:\n",
    "\n",
    "Topic #0: This topic seems to include songs that are potentially about nostalgia or enduring classics, as suggested by words like \"stay,\" \"paradise,\" and \"remastered version.\" These could be songs that have been remixed or re-released.\n",
    "\n",
    "Topic #1: The words \"dance,\" \"extended,\" \"night,\" and \"remix\" indicate this topic might be related to dance music or club tracks, with a focus on extended mixes and remixes that are commonly played in nightlife settings.\n",
    "\n",
    "Topic #2: With words like \"big,\" \"make,\" \"want,\" \"radio edit,\" and \"feat\" (featuring), this topic might encompass mainstream, radio-friendly music that often features collaborations between artists.\n",
    "\n",
    "Topic #3: The presence of words like \"forever,\" \"lost,\" \"world,\" and \"good\" suggests themes of love, loss, and perhaps introspection. The word \"feat\" again indicates collaborations, and \"black\" could refer to either a mood or be part of a band name or song title.\n",
    "\n",
    "Topic #4: This topic seems to focus on emotive content, with words like \"gonna,\" \"feel,\" \"wanna,\" \"love,\" and \"la\" (which could refer to 'the' in Spanish or shorthand for Los Angeles). The songs might be about personal feelings, desires, and relationships"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
